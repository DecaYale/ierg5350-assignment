{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IERG 5350 Assignment 2: Model-free Tabular RL\n",
    "\n",
    "*2020-2021 Term 1, IERG 5350: Reinforcement Learning. Department of Information Engineering, The Chinese University of Hong Kong. Course Instructor: Professor ZHOU Bolei. Assignment author: PENG Zhenghao, SUN Hao, ZHAN Xiaohang.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Student Name | Student ID |\n",
    "| :----: | :----: |\n",
    "| Yan XU | 1155139432 |\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welecome to the assignment 1 of our RL course. The objective of this assignment is for you to understand the classic methods used in tabular reinforcement learning. \n",
    "\n",
    "This assignment has the following sections:\n",
    "\n",
    " - Section 1: Implementation of model-free familiy of algorithms: SARSA, Q-Learning and model-free control. (100 points)\n",
    "\n",
    "You need to go through this self-contained notebook, which contains dozens of **TODOs** in part of the cells and has special `[TODO]` signs. You need to finish all TODOs. Some of them may be easy such as uncommenting a line, some of them may be difficult such as implementing a function. You can find them by searching the `[TODO]` symbol. However, we suggest you to go through the documents step by step, which will give you a better sense of the content.\n",
    "\n",
    "You are encouraged to add more code on extra cells at the end of the each section to investigate the problems you think interesting. At the end of the file, we left a place for you to optionaly write comments (Yes, please give us some either negative or positive rewards so we can keep improving the assignment!). \n",
    "\n",
    "Please report any code bugs to us via Github issues.\n",
    "\n",
    "Before you get start, remember to follow the instruction at https://github.com/cuhkrlcourse/ierg5350-assignment to setup your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: SARSA\n",
    "\n",
    "(30/100 points)\n",
    "\n",
    "You have noticed that in Assignment 1 - Section 2, we always use the function `trainer._get_transitions()` to get the transition dynamics of the environment, while never call `trainer.env.step()` to really interact with the environment. We need to access the internal feature of the environment or have somebody implement `_get_transitions` for us. However, this is not feasible in many cases, especially in some real-world cases like autonomous driving where the transition dynamics is unknown or does not explicitly exist.\n",
    "\n",
    "In this section, we will introduce the Model-free family of algorithms that do not require to know the transitions: they only get information from `env.step(action)`, that collect information by interacting with the environment rather than grab the oracle of the transition dynamics of the environment.\n",
    "\n",
    "We will continue to use the `TabularRLTrainerAbstract` class to implement algorithms, but remember you should not call `trainer._get_transitions()` anymore.\n",
    "\n",
    "We will use a simpler environment `FrozenLakerNotSlippery-v0` to conduct experiments, which has a `4 X 4` grids and is deterministic. This is because, in a model-free setting, it's extremely hard for a random agent to achieve the goal for the first time. To reduce the time of experiments, we choose to use a simpler environment. In the bonus section, you will have the chance to try model-free RL on `FrozenLake8x8-v0` to see what will happen. \n",
    "\n",
    "Now go through each section and start your coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Recall the idea of SARSA: it's an on-policy TD control method, which has distinct features compared to policy iteration and value iteration:\n",
    "\n",
    "1. Maintain a state-action pair value function $Q(s_t, a_t) = E \\sum_{i=0} \\gamma^{t+i} r_{t+i}$, namely the Q value.\n",
    "2. Do not require to know the internal dynamics of the environment.\n",
    "3. Use an epsilon-greedy policy to balance exploration and exploitation.\n",
    "\n",
    "In SARSA algorithm, we update the state action value (Q value) via TD error: \n",
    "\n",
    "$$TD(s_t, a_t) = r(s_t, a_t) + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)$$\n",
    "\n",
    "where we run the policy to get the next action $a_{t+1} = Policy(s_{t+1})$. (That's why we call SARSA an on-policy algorithm, it use the current policy to evaluate Q value).\n",
    "\n",
    "$$Q^{new}(s_t, a_t) = Q(s_t, a_t) + \\alpha TD(s_t, a_t)$$\n",
    "\n",
    "Wherein $\\alpha$ is the learning rate, a hyper-parameter provided by the user.\n",
    "\n",
    "Now go through the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Import some packages that we need to use\n",
    "from utils import *\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "def _render_helper(env):\n",
    "    env.render()\n",
    "    wait(sleep=0.2)\n",
    "\n",
    "\n",
    "def evaluate(policy, num_episodes, seed=0, env_name='FrozenLake8x8-v0', render=False):\n",
    "    \"\"\"[TODO] You need to implement this function by yourself. It\n",
    "    evaluate the given policy and return the mean episode reward.\n",
    "    We use `seed` argument for testing purpose.\n",
    "    You should pass the tests in the next cell.\n",
    "\n",
    "    :param policy: a function whose input is an interger (observation)\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: an interger, used for testing.\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag. If true, please call _render_helper\n",
    "    function.\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create environment (according to env_name, we will use env other than 'FrozenLake8x8-v0')\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Seed the environment\n",
    "    env.seed(seed)\n",
    "\n",
    "    # Build inner loop to run.\n",
    "    # For each episode, do not set the limit.\n",
    "    # Only terminate episode (reset environment) when done = True.\n",
    "    # The episode reward is the sum of all rewards happen within one episode.\n",
    "    # Call the helper function `render(env)` to render\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        # reset the environment\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        \n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            # [TODO] run the environment and terminate it if done, collect the\n",
    "            # reward at each step and sum them to the episode reward.\n",
    "            obs, reward, done, info= env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward+= reward\n",
    "            if done:\n",
    "                break;\n",
    "            pass\n",
    "        \n",
    "        rewards.append(ep_reward)\n",
    "\n",
    "    return np.mean(rewards)\n",
    "\n",
    "# [TODO] Run next cell to test your implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class TabularRLTrainerAbstract:\n",
    "    \"\"\"This is the abstract class for tabular RL trainer. We will inherent the specify \n",
    "    algorithm's trainer from this abstract class, so that we can reuse the codes like\n",
    "    getting the dynamic of the environment (self._get_transitions()) or rendering the\n",
    "    learned policy (self.render()).\"\"\"\n",
    "    \n",
    "    def __init__(self, env_name='FrozenLake8x8-v0', model_based=True):\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(self.env_name)\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.n\n",
    "        \n",
    "        self.model_based = model_based\n",
    "\n",
    "    def _get_transitions(self, state, act):\n",
    "        \"\"\"Query the environment to get the transition probability,\n",
    "        reward, the next state, and done given a pair of state and action.\n",
    "        We implement this function for you. But you need to know the \n",
    "        return format of this function.\n",
    "        \"\"\"\n",
    "        self._check_env_name()\n",
    "        assert self.model_based, \"You should not use _get_transitions in \" \\\n",
    "            \"model-free algorithm!\"\n",
    "        \n",
    "        # call the internal attribute of the environments.\n",
    "        # `transitions` is a list contain all possible next states and the \n",
    "        # probability, reward, and termination indicater corresponding to it\n",
    "        transitions = self.env.env.P[state][act]\n",
    "\n",
    "        # Given a certain state and action pair, it is possible\n",
    "        # to find there exist multiple transitions, since the \n",
    "        # environment is not deterministic.\n",
    "        # You need to know the return format of this function: a list of dicts\n",
    "        ret = []\n",
    "        for prob, next_state, reward, done in transitions:\n",
    "            ret.append({\n",
    "                \"prob\": prob,\n",
    "                \"next_state\": next_state,\n",
    "                \"reward\": reward,\n",
    "                \"done\": done\n",
    "            })\n",
    "        return ret\n",
    "    \n",
    "    def _check_env_name(self):\n",
    "        assert self.env_name.startswith('FrozenLake')\n",
    "\n",
    "    def print_table(self):\n",
    "        \"\"\"print beautiful table, only work for FrozenLake8X8-v0 env. We \n",
    "        write this function for you.\"\"\"\n",
    "        self._check_env_name()\n",
    "        print_table(self.table)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 1000 episodes when seed=0.\"\"\"\n",
    "        result = evaluate(self.policy, 1000, env_name=self.env_name)\n",
    "        return result\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Reuse your evaluate function, render current policy \n",
    "        for one episode when seed=0\"\"\"\n",
    "        evaluate(self.policy, 1, render=True, env_name=self.env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class SARSATrainer(TabularRLTrainerAbstract):\n",
    "    def __init__(self,\n",
    "                 gamma=1.0,\n",
    "                 eps=0.1,\n",
    "                 learning_rate=1.0,\n",
    "                 max_episode_length=100,\n",
    "                 env_name='FrozenLake8x8-v0'\n",
    "                 ):\n",
    "        super(SARSATrainer, self).__init__(env_name, model_based=False)\n",
    "\n",
    "        # discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # epsilon-greedy exploration policy parameter\n",
    "        self.eps = eps\n",
    "\n",
    "        # maximum steps in single episode\n",
    "        self.max_episode_length = max_episode_length\n",
    "\n",
    "        # the learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # build the Q table\n",
    "        # [TODO] uncomment the next line, pay attention to the shape\n",
    "        self.table = np.zeros((self.obs_dim, self.action_dim))\n",
    "\n",
    "    def policy(self, obs):\n",
    "        \"\"\"Implement epsilon-greedy policy\n",
    "\n",
    "        It is a function that take an integer (state / observation)\n",
    "        as input and return an interger (action).\n",
    "        \"\"\"\n",
    "\n",
    "        # [TODO] You need to implement the epsilon-greedy policy here.\n",
    "        # hint: We have self.eps probability to choose a unifomly random\n",
    "        #  action in range [0, 1, .., self.action_dim - 1], \n",
    "        #  otherwise choose action that maximize the Q value\n",
    "        if np.random.rand()<self.eps:\n",
    "            act=np.random.choice(range(self.action_dim))\n",
    "        else:\n",
    "            act = np.argmax(self.table[obs])\n",
    "        return act\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        # [TODO] Q table may be need to be reset to zeros.\n",
    "        # if you think it should, than do it. If not, then move on.\n",
    "        pass\n",
    "        # No, we should do nothing.\n",
    "\n",
    "        obs = self.env.reset()\n",
    "        for t in range(self.max_episode_length):\n",
    "            act = self.policy(obs)\n",
    "\n",
    "            next_obs, reward, done, _ = self.env.step(act)\n",
    "            next_act = self.policy(next_obs)\n",
    "\n",
    "            # [TODO] compute the TD error, based on the next observation and\n",
    "            #  action.\n",
    "            td_error = None\n",
    "            \n",
    "            td_error = reward+self.gamma*self.table[next_obs][next_act]-self.table[obs][act]\n",
    "            pass\n",
    "\n",
    "            # [TODO] compute the new Q value\n",
    "            # hint: use TD error, self.learning_rate and old Q value\n",
    "            new_value = None\n",
    "            new_value=self.table[obs][act]+self.learning_rate*td_error\n",
    "            pass\n",
    "\n",
    "            self.table[obs][act] = new_value\n",
    "\n",
    "            # [TODO] Implement (1) break if done. (2) update obs for next \n",
    "            #  self.policy(obs) call\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                obs=next_obs\n",
    "            pass\n",
    "\n",
    "# [TODO] run the next cell to check your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have finish the SARSA trainer. To make sure your implementation of epsilon-greedy strategy is correct, please run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# set eps = 0 to disable exploration.\n",
    "test_trainer = SARSATrainer(eps=0.0)\n",
    "test_trainer.table.fill(0)\n",
    "\n",
    "# set the Q value of (obs 0, act 3) to 100, so that it should be taken by \n",
    "# policy.\n",
    "test_obs = 0\n",
    "test_act = test_trainer.action_dim - 1\n",
    "test_trainer.table[test_obs][test_act] = 100\n",
    "\n",
    "# assertion\n",
    "assert test_trainer.policy(test_obs) == test_act, \\\n",
    "    \"Your action is wrong! Should be {} but get {}.\".format(\n",
    "        test_act, test_trainer.policy(test_obs))\n",
    "\n",
    "# delete trainer\n",
    "del test_trainer\n",
    "\n",
    "# set eps = 0 to disable exploitation.\n",
    "test_trainer = SARSATrainer(eps=1.0)\n",
    "test_trainer.table.fill(0)\n",
    "\n",
    "act_set = set()\n",
    "for i in range(100):\n",
    "    act_set.add(test_trainer.policy(0))\n",
    "\n",
    "# assertion\n",
    "assert len(act_set) > 1, (\"You sure your uniformaly action selection mechanism\"\n",
    "                          \" is working? You only take action {} when \"\n",
    "                          \"observation is 0, though we run trainer.policy() \"\n",
    "                          \"for 100 times.\".format(act_set))\n",
    "# delete trainer\n",
    "del test_trainer\n",
    "\n",
    "print(\"Policy Test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the next cell to see the result. Note that we use the non-slippery version of a small frozen lake environment `FrozenLakeNotSlipppery-v0` (this is not a ready Gym environment, see `utils.py` for details). This is because, in the model-free setting, it's extremely hard to access the goal for the first time (you should already know that if you watch the agent randomly acting in Assignment 1 - Section 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve TODO\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_sarsa_config = dict(\n",
    "    max_iteration=20000,\n",
    "    max_episode_length=200,\n",
    "    learning_rate=0.01,\n",
    "    evaluate_interval=1000,\n",
    "    gamma=0.8,\n",
    "    eps=0.3,\n",
    "    env_name='FrozenLakeNotSlippery-v0'\n",
    ")\n",
    "\n",
    "\n",
    "def sarsa(train_config=None):\n",
    "    config = default_sarsa_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    trainer = SARSATrainer(\n",
    "        gamma=config['gamma'],\n",
    "        eps=config['eps'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        max_episode_length=config['max_episode_length'],\n",
    "        env_name=config['env_name']\n",
    "    )\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "        # train the agent\n",
    "        trainer.train()  # [TODO] please uncomment this line\n",
    "\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "\n",
    "    if trainer.evaluate() < 0.6:\n",
    "        print(\"We expect to get the mean episode reward greater than 0.6. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate()))\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.001.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.001.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.002.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.002.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.639.\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 0.67.\n",
      "[INFO]\tIn 11000 iteration, current mean episode reward is 0.669.\n",
      "[INFO]\tIn 12000 iteration, current mean episode reward is 0.645.\n",
      "[INFO]\tIn 13000 iteration, current mean episode reward is 0.648.\n",
      "[INFO]\tIn 14000 iteration, current mean episode reward is 0.664.\n",
      "[INFO]\tIn 15000 iteration, current mean episode reward is 0.662.\n",
      "[INFO]\tIn 16000 iteration, current mean episode reward is 0.674.\n",
      "[INFO]\tIn 17000 iteration, current mean episode reward is 0.662.\n",
      "[INFO]\tIn 18000 iteration, current mean episode reward is 0.646.\n",
      "[INFO]\tIn 19000 iteration, current mean episode reward is 0.678.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "sarsa_trainer = sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== The state value for action 0 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.124|0.122|0.047|0.002|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.173|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.247|0.237|0.237|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.000|0.505|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 1 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.165|0.000|0.002|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.236|0.000|0.224|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.000|0.477|0.730|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.517|0.731|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 2 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.079|0.010|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.000|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.334|0.462|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.726|1.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 3 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.124|0.043|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.125|0.000|0.001|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.162|0.000|0.050|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.345|0.473|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "sarsa_trainer.print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "sarsa_trainer.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have finished the SARSA algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Q-Learning\n",
    "(30/100 points)\n",
    "\n",
    "Q-learning is an off-policy algorithm who differs from SARSA in the computing of TD error. Instead of running policy to get `next_act` $a'$ and get the TD error by:\n",
    "\n",
    "$r + \\gamma Q(s', a') - Q(s, a)$, \n",
    "\n",
    "in Q-learning we compute the TD error via:\n",
    "\n",
    "$r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$. \n",
    "\n",
    "The reason we call it \"off-policy\" is that the policy involves the computing of next-Q value is not the \"behavior policy\", instead, it is a \"virtural policy\" that always takes the best action given current Q values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class QLearningTrainer(TabularRLTrainerAbstract):\n",
    "    def __init__(self,\n",
    "                 gamma=1.0,\n",
    "                 eps=0.1,\n",
    "                 learning_rate=1.0,\n",
    "                 max_episode_length=100,\n",
    "                 env_name='FrozenLake8x8-v0'\n",
    "                 ):\n",
    "        super(QLearningTrainer, self).__init__(env_name, model_based=False)\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # build the Q table\n",
    "        self.table = np.zeros((self.obs_dim, self.action_dim))\n",
    "\n",
    "    def policy(self, obs):\n",
    "        \"\"\"Implement epsilon-greedy policy\n",
    "\n",
    "        It is a function that take an integer (state / observation)\n",
    "        as input and return an interger (action).\n",
    "        \"\"\"\n",
    "\n",
    "        # [TODO] You need to implement the epsilon-greedy policy here.\n",
    "        # hint: Just copy your codes in SARSATrainer.policy()\n",
    "        if np.random.rand()<self.eps:\n",
    "            act=np.random.choice(range(self.action_dim))\n",
    "        else:\n",
    "            act = np.argmax(self.table[obs])\n",
    "        return act\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        # [TODO] Q table may be need to be reset to zeros.\n",
    "        # if you think it should, than do it. If not, then move on.\n",
    "        pass\n",
    "        # No, we should do nothing.\n",
    "\n",
    "        obs = self.env.reset()\n",
    "        for t in range(self.max_episode_length):\n",
    "            act = self.policy(obs)\n",
    "\n",
    "            next_obs, reward, done, _ = self.env.step(act)\n",
    "\n",
    "            # [TODO] compute the TD error, based on the next observation\n",
    "            # hint: we do not need next_act anymore.\n",
    "            td_error = None\n",
    "            \n",
    "            td_error = reward+self.gamma*self.table[next_obs][np.argmax(self.table[next_obs])]-self.table[obs][act]\n",
    "            pass\n",
    "\n",
    "            # [TODO] compute the new Q value\n",
    "            # hint: use TD error, self.learning_rate and old Q value\n",
    "            new_value = None\n",
    "            new_value=self.table[obs][act]+self.learning_rate*td_error\n",
    "            pass\n",
    "\n",
    "            self.table[obs][act] = new_value\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODO\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_q_learning_config = dict(\n",
    "    max_iteration=20000,\n",
    "    max_episode_length=200,\n",
    "    learning_rate=0.01,\n",
    "    evaluate_interval=1000,\n",
    "    gamma=0.8,\n",
    "    eps=0.3,\n",
    "    env_name='FrozenLakeNotSlippery-v0'\n",
    ")\n",
    "\n",
    "\n",
    "def q_learning(train_config=None):\n",
    "    config = default_q_learning_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    trainer = QLearningTrainer(\n",
    "        gamma=config['gamma'],\n",
    "        eps=config['eps'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        max_episode_length=config['max_episode_length'],\n",
    "        env_name=config['env_name']\n",
    "    )\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "        # train the agent\n",
    "        trainer.train()  # [TODO] please uncomment this line\n",
    "\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "\n",
    "    if trainer.evaluate() < 0.6:\n",
    "        print(\"We expect to get the mean episode reward greater than 0.6. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate()))\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.001.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.001.\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 0.002.\n",
      "[INFO]\tIn 11000 iteration, current mean episode reward is 0.666.\n",
      "[INFO]\tIn 12000 iteration, current mean episode reward is 0.66.\n",
      "[INFO]\tIn 13000 iteration, current mean episode reward is 0.651.\n",
      "[INFO]\tIn 14000 iteration, current mean episode reward is 0.663.\n",
      "[INFO]\tIn 15000 iteration, current mean episode reward is 0.683.\n",
      "[INFO]\tIn 16000 iteration, current mean episode reward is 0.643.\n",
      "[INFO]\tIn 17000 iteration, current mean episode reward is 0.669.\n",
      "[INFO]\tIn 18000 iteration, current mean episode reward is 0.636.\n",
      "[INFO]\tIn 19000 iteration, current mean episode reward is 0.676.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "q_learning_trainer = q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== The state value for action 0 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.262|0.262|0.121|0.003|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.328|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.409|0.409|0.282|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.000|0.639|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 1 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.328|0.000|0.003|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.410|0.000|0.289|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.000|0.640|0.800|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.638|0.798|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 2 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.209|0.036|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.000|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.512|0.637|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.800|1.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 3 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.262|0.117|0.004|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.262|0.000|0.003|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.327|0.000|0.068|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.510|0.635|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "q_learning_trainer.print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "q_learning_trainer.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have finished Q-Learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Monte Carlo Control\n",
    "(40/100 points)\n",
    "\n",
    "In sections 1 and 2, we implement the on-policy and off-policy versions of the TD Learning algorithms. In this section, we will play with another branch of the model-free algorithm: Monte Carlo Control. You can refer to the 5.3 Monte Carlo Control section of the textbook \"Reinforcement Learning: An Introduction\" to learn the details of MC control.\n",
    "\n",
    "The basic idea of MC control is to compute the Q value (state-action value) directly from an episode, without using TD to fit the Q function. Concretely, we maintain a batch of lists (the total number of lists is `obs_dim * action_dim`), each elememnt of the batch is a list correspondent to a state-action pair. The list is used to store the previously happenning \"return\" of each state action pair.\n",
    "\n",
    "We will use a dict `self.returns` to store all lists. The keys of the dict are tuples `(obs, act)`: `self.returns[(obs, act)]` is the list to store all returns when `(obs, act)` happens. \n",
    "\n",
    "The key point of MC Control method is that we take the mean of this list (the mean of all previous returns) as the Q value of this state-action pair.\n",
    "\n",
    "The \"return\" here is the discounted return starting from the state-action pair: $Return(s_t, a_t) = \\sum_{i=0} \\gamma^{t+i} r_{t+i}$.\n",
    "\n",
    "In short, MC Control method uses a new way to estimate the values of state-action pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class MCControlTrainer(TabularRLTrainerAbstract):\n",
    "    def __init__(self,\n",
    "                 gamma=1.0,\n",
    "                 eps=0.3,\n",
    "                 max_episode_length=100,\n",
    "                 env_name='FrozenLake8x8-v0'\n",
    "                 ):\n",
    "        super(MCControlTrainer, self).__init__(env_name, model_based=False)\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.max_episode_length = max_episode_length\n",
    "\n",
    "        # build the dict of lists\n",
    "        self.returns = {}\n",
    "        for obs in range(self.obs_dim):\n",
    "            for act in range(self.action_dim):\n",
    "                self.returns[(obs, act)] = []\n",
    "\n",
    "        # build the Q table\n",
    "        self.table = np.zeros((self.obs_dim, self.action_dim))\n",
    "\n",
    "    def policy(self, obs):\n",
    "        \"\"\"Implement epsilon-greedy policy\n",
    "\n",
    "        It is a function that take an integer (state / observation)\n",
    "        as input and return an interger (action).\n",
    "        \"\"\"\n",
    "\n",
    "        # [TODO] You need to implement the epsilon-greedy policy here.\n",
    "        # hint: Just copy your codes in SARSATrainer.policy()\n",
    "        action = None\n",
    "        if np.random.rand()<self.eps:\n",
    "            act=np.random.choice(range(self.action_dim))\n",
    "        else:\n",
    "            act = np.argmax(self.table[obs])\n",
    "        return act\n",
    "        pass\n",
    "            \n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # [TODO] rollout for one episode, store data in three lists create \n",
    "        #  above.\n",
    "        # hint: we do not need to store next observation.\n",
    "        \n",
    "        obs = self.env.reset()\n",
    "        for t in range(self.max_episode_length):\n",
    "            act = self.policy(obs)\n",
    "\n",
    "            next_obs, reward, done, _ = self.env.step(act)\n",
    "            observations.append(obs)\n",
    "            actions.append(act)\n",
    "            rewards.append(reward)\n",
    "            obs=next_obs\n",
    "            if done:\n",
    "                break\n",
    "        pass\n",
    "\n",
    "        assert len(actions) == len(observations)\n",
    "        assert len(actions) == len(rewards)\n",
    "\n",
    "        occured_state_action_pair = set()\n",
    "        length = len(actions)\n",
    "        value = 0\n",
    "        for i in reversed(range(length)):\n",
    "            # if length = 10, then i = 9, 8, ..., 0\n",
    "\n",
    "            obs = observations[i]\n",
    "            act = actions[i]\n",
    "            reward = rewards[i]\n",
    "\n",
    "            # [TODO] compute the value reversely\n",
    "            # hint: value(t) = gamma * value(t+1) + r(t)\n",
    "            value=self.gamma*value+reward\n",
    "            pass\n",
    "\n",
    "            if (obs, act) not in occured_state_action_pair:\n",
    "                occured_state_action_pair.add((obs, act))\n",
    "\n",
    "                # [TODO] append current return (value) to dict\n",
    "                # hint: `value` represents the future return due to \n",
    "                #  current (obs, act), so we need to store this value\n",
    "                #  in trainer.returns\n",
    "                self.returns[(obs,act)].append(value)\n",
    "                pass\n",
    "\n",
    "                # [TODO] compute the Q value from self.returns and write it \n",
    "                #  into self.table\n",
    "                self.table[obs,act] =  np.mean(self.returns[(obs,act)])\n",
    "                pass\n",
    "\n",
    "                # we don't need to update the policy since it is \n",
    "    \n",
    "                # automatically adjusted with self.table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_mc_control_config = dict(\n",
    "    max_iteration=20000,\n",
    "    max_episode_length=200,\n",
    "    evaluate_interval=1000,\n",
    "    gamma=0.8,\n",
    "    eps=0.3,\n",
    "    env_name='FrozenLakeNotSlippery-v0'\n",
    ")\n",
    "\n",
    "\n",
    "def mc_control(train_config=None):\n",
    "    config = default_mc_control_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    trainer = MCControlTrainer(\n",
    "        gamma=config['gamma'],\n",
    "        eps=config['eps'],\n",
    "        max_episode_length=config['max_episode_length'],\n",
    "        env_name=config['env_name']\n",
    "    )\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "        # train the agent\n",
    "        trainer.train()\n",
    "\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "\n",
    "    if trainer.evaluate() < 0.6:\n",
    "        print(\"We expect to get the mean episode reward greater than 0.6. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate()))\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 0.001.\n",
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.656.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.645.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.651.\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.654.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.66.\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.645.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.665.\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 0.655.\n",
      "[INFO]\tIn 11000 iteration, current mean episode reward is 0.655.\n",
      "[INFO]\tIn 12000 iteration, current mean episode reward is 0.673.\n",
      "[INFO]\tIn 13000 iteration, current mean episode reward is 0.65.\n",
      "[INFO]\tIn 14000 iteration, current mean episode reward is 0.674.\n",
      "[INFO]\tIn 15000 iteration, current mean episode reward is 0.666.\n",
      "[INFO]\tIn 16000 iteration, current mean episode reward is 0.673.\n",
      "[INFO]\tIn 17000 iteration, current mean episode reward is 0.651.\n",
      "[INFO]\tIn 18000 iteration, current mean episode reward is 0.674.\n",
      "[INFO]\tIn 19000 iteration, current mean episode reward is 0.675.\n",
      "[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.661.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.649.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.674.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.664.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.685.\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.671.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.66.\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.684.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.664.\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 0.653.\n",
      "[INFO]\tIn 11000 iteration, current mean episode reward is 0.645.\n",
      "[INFO]\tIn 12000 iteration, current mean episode reward is 0.678.\n",
      "[INFO]\tIn 13000 iteration, current mean episode reward is 0.654.\n",
      "[INFO]\tIn 14000 iteration, current mean episode reward is 0.658.\n",
      "[INFO]\tIn 15000 iteration, current mean episode reward is 0.632.\n",
      "[INFO]\tIn 16000 iteration, current mean episode reward is 0.686.\n",
      "[INFO]\tIn 17000 iteration, current mean episode reward is 0.682.\n",
      "[INFO]\tIn 18000 iteration, current mean episode reward is 0.64.\n",
      "[INFO]\tIn 19000 iteration, current mean episode reward is 0.645.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "mc_control_trainer = mc_control()\n",
    "\n",
    "sarsa_trainer = sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== The state value for action 0 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.059|0.017|0.058|0.222|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.077|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.158|0.193|0.342|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.000|0.514|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 1 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.155|0.000|0.327|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.234|0.000|0.503|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.000|0.478|0.750|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.516|0.733|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 2 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.086|0.201|0.153|0.087|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.000|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.349|0.504|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.744|1.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 3 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.073|0.059|0.226|0.149|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.087|0.000|0.243|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.130|0.000|0.318|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.353|0.503|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "mc_control_trainer.print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "mc_control_trainer.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secion 4 Bonus (optional): Tune and train FrozenLake8x8-v0 with Model-free algorithms\n",
    "\n",
    "You have noticed that we use a simpler environment `FrozenLakeNotSlippery-v0` which has only 16 states and is not stochastic. Can you try to train Model-free families of algorithm using the `FrozenLake8x8-v0` environment? Tune the hyperparameters and compare the results between different algorithms.\n",
    "\n",
    "Hint: It's not easy to train model-free algorithm in `FrozenLake8x8-v0`. Failure is excepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_mc_control_config = dict(\n",
    "    max_iteration=20000,\n",
    "    max_episode_length=200,\n",
    "    evaluate_interval=1000,\n",
    "    gamma=0.8,\n",
    "    eps=0.3,\n",
    "    env_name='FrozenLakeNotSlippery-v0'\n",
    ")\n",
    "\n",
    "\n",
    "def mc_control(train_config=None):\n",
    "    config = default_mc_control_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    trainer = MCControlTrainer(\n",
    "        gamma=config['gamma'],\n",
    "        eps=config['eps'],\n",
    "        max_episode_length=config['max_episode_length'],\n",
    "        env_name=config['env_name']\n",
    "    )\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "        # train the agent\n",
    "        trainer.train()\n",
    "        if i%1000==0:\n",
    "            trainer.eps*=0.95\n",
    "            print(trainer.eps)\n",
    "\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "\n",
    "    if trainer.evaluate() < 0.6:\n",
    "        print(\"We expect to get the mean episode reward greater than 0.6. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate()))\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# Solve the TODO\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_q_learning_config = dict(\n",
    "    max_iteration=20000,\n",
    "    max_episode_length=200,\n",
    "    learning_rate=0.01,\n",
    "    evaluate_interval=1000,\n",
    "    gamma=0.8,\n",
    "    eps=0.3,\n",
    "    env_name='FrozenLakeNotSlippery-v0'\n",
    ")\n",
    "\n",
    "\n",
    "def q_learning(train_config=None):\n",
    "    config = default_q_learning_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    trainer = QLearningTrainer(\n",
    "        gamma=config['gamma'],\n",
    "        eps=config['eps'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        max_episode_length=config['max_episode_length'],\n",
    "        env_name=config['env_name']\n",
    "    )\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "        # train the agent\n",
    "        trainer.train()  # [TODO] please uncomment this line\n",
    "        if i%1000==0:\n",
    "            trainer.eps*=0.95\n",
    "            \n",
    "            print(trainer.eps)\n",
    "            \n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            eval_result=trainer.evaluate()\n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "            if eval_result>0.01:\n",
    "                trainer.learning_rate=0.05\n",
    "            elif eval_result>0.1:\n",
    "                trainer.learning_rate=0.1\n",
    "                trainer.eps=0.1\n",
    "            elif eval_result>0.3:\n",
    "                trainer.learning_rate=0.01\n",
    "                trainer.eps=0.01\n",
    "\n",
    "    if trainer.evaluate() < 0.6:\n",
    "        print(\"We expect to get the mean episode reward greater than 0.6. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate()))\n",
    "\n",
    "    return trainer\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_sarsa_config = dict(\n",
    "    max_iteration=20000,\n",
    "    max_episode_length=200,\n",
    "    learning_rate=0.01,\n",
    "    evaluate_interval=1000,\n",
    "    gamma=0.8,\n",
    "    eps=0.3,\n",
    "    env_name='FrozenLakeNotSlippery-v0'\n",
    ")\n",
    "\n",
    "\n",
    "def sarsa(train_config=None):\n",
    "    config = default_sarsa_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    trainer = SARSATrainer(\n",
    "        gamma=config['gamma'],\n",
    "        eps=config['eps'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        max_episode_length=config['max_episode_length'],\n",
    "        env_name=config['env_name']\n",
    "    )\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "        # train the agent\n",
    "        trainer.train()  # [TODO] please uncomment this line\n",
    "        if i%2000==0 and trainer.eps>0.1:\n",
    "            trainer.eps*=0.95\n",
    "            \n",
    "            print(trainer.eps)\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            eval_result=trainer.evaluate()\n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, eval_result))\n",
    "            if eval_result>0.01:\n",
    "                trainer.learning_rate=0.1\n",
    "            elif eval_result>0.1:\n",
    "                trainer.learning_rate=0.1\n",
    "                trainer.eps=0.1\n",
    "            elif eval_result>0.3:\n",
    "                trainer.learning_rate=0.01\n",
    "                trainer.eps=0.01\n",
    "\n",
    "    if trainer.evaluate() < 0.6:\n",
    "        print(\"We expect to get the mean episode reward greater than 0.6. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate()))\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9405\n",
      "[INFO]\tIn 0 iteration, current mean episode reward is 0.002.\n",
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.001.\n",
      "0.8934749999999999\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.003.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.001.\n",
      "0.8488012499999998\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.001.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.0.\n",
      "0.8063611874999999\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.001.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.001.\n",
      "0.7660431281249999\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.003.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.007.\n",
      "0.7277409717187499\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 0.015.\n",
      "[INFO]\tIn 11000 iteration, current mean episode reward is 0.011.\n",
      "0.6913539231328123\n",
      "[INFO]\tIn 12000 iteration, current mean episode reward is 0.016.\n",
      "[INFO]\tIn 13000 iteration, current mean episode reward is 0.007.\n",
      "0.6567862269761716\n",
      "[INFO]\tIn 14000 iteration, current mean episode reward is 0.02.\n",
      "[INFO]\tIn 15000 iteration, current mean episode reward is 0.016.\n",
      "0.623946915627363\n",
      "[INFO]\tIn 16000 iteration, current mean episode reward is 0.02.\n",
      "[INFO]\tIn 17000 iteration, current mean episode reward is 0.015.\n",
      "0.5927495698459949\n",
      "[INFO]\tIn 18000 iteration, current mean episode reward is 0.022.\n",
      "[INFO]\tIn 19000 iteration, current mean episode reward is 0.014.\n",
      "0.5631120913536951\n",
      "[INFO]\tIn 20000 iteration, current mean episode reward is 0.023.\n",
      "[INFO]\tIn 21000 iteration, current mean episode reward is 0.017.\n",
      "0.5349564867860104\n",
      "[INFO]\tIn 22000 iteration, current mean episode reward is 0.025.\n",
      "[INFO]\tIn 23000 iteration, current mean episode reward is 0.03.\n",
      "0.5082086624467098\n",
      "[INFO]\tIn 24000 iteration, current mean episode reward is 0.044.\n",
      "[INFO]\tIn 25000 iteration, current mean episode reward is 0.021.\n",
      "0.4827982293243743\n",
      "[INFO]\tIn 26000 iteration, current mean episode reward is 0.018.\n",
      "[INFO]\tIn 27000 iteration, current mean episode reward is 0.023.\n",
      "0.45865831785815553\n",
      "[INFO]\tIn 28000 iteration, current mean episode reward is 0.049.\n",
      "[INFO]\tIn 29000 iteration, current mean episode reward is 0.052.\n",
      "0.4357254019652477\n",
      "[INFO]\tIn 30000 iteration, current mean episode reward is 0.071.\n",
      "[INFO]\tIn 31000 iteration, current mean episode reward is 0.046.\n",
      "0.41393913186698533\n",
      "[INFO]\tIn 32000 iteration, current mean episode reward is 0.041.\n",
      "[INFO]\tIn 33000 iteration, current mean episode reward is 0.055.\n",
      "0.39324217527363603\n",
      "[INFO]\tIn 34000 iteration, current mean episode reward is 0.019.\n",
      "[INFO]\tIn 35000 iteration, current mean episode reward is 0.046.\n",
      "0.37358006650995423\n",
      "[INFO]\tIn 36000 iteration, current mean episode reward is 0.05.\n",
      "[INFO]\tIn 37000 iteration, current mean episode reward is 0.065.\n",
      "0.3549010631844565\n",
      "[INFO]\tIn 38000 iteration, current mean episode reward is 0.047.\n",
      "[INFO]\tIn 39000 iteration, current mean episode reward is 0.06.\n",
      "0.33715601002523365\n",
      "[INFO]\tIn 40000 iteration, current mean episode reward is 0.06.\n",
      "[INFO]\tIn 41000 iteration, current mean episode reward is 0.064.\n",
      "0.320298209523972\n",
      "[INFO]\tIn 42000 iteration, current mean episode reward is 0.094.\n",
      "[INFO]\tIn 43000 iteration, current mean episode reward is 0.071.\n",
      "0.30428329904777335\n",
      "[INFO]\tIn 44000 iteration, current mean episode reward is 0.163.\n",
      "[INFO]\tIn 45000 iteration, current mean episode reward is 0.088.\n",
      "0.28906913409538465\n",
      "[INFO]\tIn 46000 iteration, current mean episode reward is 0.135.\n",
      "[INFO]\tIn 47000 iteration, current mean episode reward is 0.066.\n",
      "0.27461567739061543\n",
      "[INFO]\tIn 48000 iteration, current mean episode reward is 0.081.\n",
      "[INFO]\tIn 49000 iteration, current mean episode reward is 0.177.\n",
      "0.2608848935210846\n",
      "[INFO]\tIn 50000 iteration, current mean episode reward is 0.07.\n",
      "[INFO]\tIn 51000 iteration, current mean episode reward is 0.15.\n",
      "0.24784064884503038\n",
      "[INFO]\tIn 52000 iteration, current mean episode reward is 0.16.\n",
      "[INFO]\tIn 53000 iteration, current mean episode reward is 0.078.\n",
      "0.23544861640277884\n",
      "[INFO]\tIn 54000 iteration, current mean episode reward is 0.159.\n",
      "[INFO]\tIn 55000 iteration, current mean episode reward is 0.112.\n",
      "0.22367618558263988\n",
      "[INFO]\tIn 56000 iteration, current mean episode reward is 0.154.\n",
      "[INFO]\tIn 57000 iteration, current mean episode reward is 0.116.\n",
      "0.21249237630350787\n",
      "[INFO]\tIn 58000 iteration, current mean episode reward is 0.111.\n",
      "[INFO]\tIn 59000 iteration, current mean episode reward is 0.13.\n",
      "0.20186775748833247\n",
      "[INFO]\tIn 60000 iteration, current mean episode reward is 0.153.\n",
      "[INFO]\tIn 61000 iteration, current mean episode reward is 0.167.\n",
      "0.19177436961391583\n",
      "[INFO]\tIn 62000 iteration, current mean episode reward is 0.143.\n",
      "[INFO]\tIn 63000 iteration, current mean episode reward is 0.177.\n",
      "0.18218565113322002\n",
      "[INFO]\tIn 64000 iteration, current mean episode reward is 0.168.\n",
      "[INFO]\tIn 65000 iteration, current mean episode reward is 0.1.\n",
      "0.173076368576559\n",
      "[INFO]\tIn 66000 iteration, current mean episode reward is 0.314.\n",
      "[INFO]\tIn 67000 iteration, current mean episode reward is 0.058.\n",
      "0.16442255014773105\n",
      "[INFO]\tIn 68000 iteration, current mean episode reward is 0.291.\n",
      "[INFO]\tIn 69000 iteration, current mean episode reward is 0.177.\n",
      "0.15620142264034448\n",
      "[INFO]\tIn 70000 iteration, current mean episode reward is 0.389.\n",
      "[INFO]\tIn 71000 iteration, current mean episode reward is 0.22.\n",
      "0.14839135150832725\n",
      "[INFO]\tIn 72000 iteration, current mean episode reward is 0.24.\n",
      "[INFO]\tIn 73000 iteration, current mean episode reward is 0.291.\n",
      "0.14097178393291088\n",
      "[INFO]\tIn 74000 iteration, current mean episode reward is 0.357.\n",
      "[INFO]\tIn 75000 iteration, current mean episode reward is 0.185.\n",
      "0.13392319473626532\n",
      "[INFO]\tIn 76000 iteration, current mean episode reward is 0.213.\n",
      "[INFO]\tIn 77000 iteration, current mean episode reward is 0.202.\n",
      "0.12722703499945204\n",
      "[INFO]\tIn 78000 iteration, current mean episode reward is 0.232.\n",
      "[INFO]\tIn 79000 iteration, current mean episode reward is 0.238.\n",
      "0.12086568324947942\n",
      "[INFO]\tIn 80000 iteration, current mean episode reward is 0.253.\n",
      "[INFO]\tIn 81000 iteration, current mean episode reward is 0.216.\n",
      "0.11482239908700545\n",
      "[INFO]\tIn 82000 iteration, current mean episode reward is 0.345.\n",
      "[INFO]\tIn 83000 iteration, current mean episode reward is 0.143.\n",
      "0.10908127913265517\n",
      "[INFO]\tIn 84000 iteration, current mean episode reward is 0.201.\n",
      "[INFO]\tIn 85000 iteration, current mean episode reward is 0.187.\n",
      "0.1036272151760224\n",
      "[INFO]\tIn 86000 iteration, current mean episode reward is 0.253.\n",
      "[INFO]\tIn 87000 iteration, current mean episode reward is 0.253.\n",
      "0.09844585441722127\n",
      "[INFO]\tIn 88000 iteration, current mean episode reward is 0.323.\n",
      "[INFO]\tIn 89000 iteration, current mean episode reward is 0.393.\n",
      "0.0935235616963602\n",
      "[INFO]\tIn 90000 iteration, current mean episode reward is 0.351.\n",
      "[INFO]\tIn 91000 iteration, current mean episode reward is 0.316.\n",
      "0.08884738361154218\n",
      "[INFO]\tIn 92000 iteration, current mean episode reward is 0.239.\n",
      "[INFO]\tIn 93000 iteration, current mean episode reward is 0.269.\n",
      "0.08440501443096507\n",
      "[INFO]\tIn 94000 iteration, current mean episode reward is 0.42.\n",
      "[INFO]\tIn 95000 iteration, current mean episode reward is 0.536.\n",
      "0.08018476370941681\n",
      "[INFO]\tIn 96000 iteration, current mean episode reward is 0.186.\n",
      "[INFO]\tIn 97000 iteration, current mean episode reward is 0.28.\n",
      "0.07617552552394596\n",
      "[INFO]\tIn 98000 iteration, current mean episode reward is 0.215.\n",
      "[INFO]\tIn 99000 iteration, current mean episode reward is 0.405.\n",
      "0.07236674924774866\n",
      "[INFO]\tIn 100000 iteration, current mean episode reward is 0.323.\n",
      "[INFO]\tIn 101000 iteration, current mean episode reward is 0.382.\n",
      "0.06874841178536123\n",
      "[INFO]\tIn 102000 iteration, current mean episode reward is 0.359.\n",
      "[INFO]\tIn 103000 iteration, current mean episode reward is 0.271.\n",
      "0.06531099119609317\n",
      "[INFO]\tIn 104000 iteration, current mean episode reward is 0.492.\n",
      "[INFO]\tIn 105000 iteration, current mean episode reward is 0.487.\n",
      "0.06204544163628851\n",
      "[INFO]\tIn 106000 iteration, current mean episode reward is 0.212.\n",
      "[INFO]\tIn 107000 iteration, current mean episode reward is 0.428.\n",
      "0.05894316955447408\n",
      "[INFO]\tIn 108000 iteration, current mean episode reward is 0.347.\n",
      "[INFO]\tIn 109000 iteration, current mean episode reward is 0.569.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.055996011076750375\n",
      "[INFO]\tIn 110000 iteration, current mean episode reward is 0.384.\n",
      "[INFO]\tIn 111000 iteration, current mean episode reward is 0.129.\n",
      "0.05319621052291285\n",
      "[INFO]\tIn 112000 iteration, current mean episode reward is 0.256.\n",
      "[INFO]\tIn 113000 iteration, current mean episode reward is 0.134.\n",
      "0.050536399996767206\n",
      "[INFO]\tIn 114000 iteration, current mean episode reward is 0.441.\n",
      "[INFO]\tIn 115000 iteration, current mean episode reward is 0.363.\n",
      "0.04800957999692884\n",
      "[INFO]\tIn 116000 iteration, current mean episode reward is 0.402.\n",
      "[INFO]\tIn 117000 iteration, current mean episode reward is 0.321.\n",
      "0.045609100997082395\n",
      "[INFO]\tIn 118000 iteration, current mean episode reward is 0.22.\n",
      "[INFO]\tIn 119000 iteration, current mean episode reward is 0.427.\n",
      "0.043328645947228274\n",
      "[INFO]\tIn 120000 iteration, current mean episode reward is 0.276.\n",
      "[INFO]\tIn 121000 iteration, current mean episode reward is 0.415.\n",
      "0.04116221364986686\n",
      "[INFO]\tIn 122000 iteration, current mean episode reward is 0.381.\n",
      "[INFO]\tIn 123000 iteration, current mean episode reward is 0.429.\n",
      "0.039104102967373516\n",
      "[INFO]\tIn 124000 iteration, current mean episode reward is 0.405.\n",
      "[INFO]\tIn 125000 iteration, current mean episode reward is 0.443.\n",
      "0.037148897819004836\n",
      "[INFO]\tIn 126000 iteration, current mean episode reward is 0.35.\n",
      "[INFO]\tIn 127000 iteration, current mean episode reward is 0.393.\n",
      "0.03529145292805459\n",
      "[INFO]\tIn 128000 iteration, current mean episode reward is 0.394.\n",
      "[INFO]\tIn 129000 iteration, current mean episode reward is 0.449.\n",
      "0.033526880281651864\n",
      "[INFO]\tIn 130000 iteration, current mean episode reward is 0.589.\n",
      "[INFO]\tIn 131000 iteration, current mean episode reward is 0.335.\n",
      "0.03185053626756927\n",
      "[INFO]\tIn 132000 iteration, current mean episode reward is 0.438.\n",
      "[INFO]\tIn 133000 iteration, current mean episode reward is 0.261.\n",
      "0.030258009454190805\n",
      "[INFO]\tIn 134000 iteration, current mean episode reward is 0.601.\n",
      "[INFO]\tIn 135000 iteration, current mean episode reward is 0.356.\n",
      "0.028745108981481263\n",
      "[INFO]\tIn 136000 iteration, current mean episode reward is 0.249.\n",
      "[INFO]\tIn 137000 iteration, current mean episode reward is 0.284.\n",
      "0.027307853532407198\n",
      "[INFO]\tIn 138000 iteration, current mean episode reward is 0.26.\n",
      "[INFO]\tIn 139000 iteration, current mean episode reward is 0.633.\n",
      "0.025942460855786838\n",
      "[INFO]\tIn 140000 iteration, current mean episode reward is 0.575.\n",
      "[INFO]\tIn 141000 iteration, current mean episode reward is 0.393.\n",
      "0.024645337812997496\n",
      "[INFO]\tIn 142000 iteration, current mean episode reward is 0.248.\n",
      "[INFO]\tIn 143000 iteration, current mean episode reward is 0.627.\n",
      "0.02341307092234762\n",
      "[INFO]\tIn 144000 iteration, current mean episode reward is 0.591.\n",
      "[INFO]\tIn 145000 iteration, current mean episode reward is 0.411.\n",
      "0.02224241737623024\n",
      "[INFO]\tIn 146000 iteration, current mean episode reward is 0.514.\n",
      "[INFO]\tIn 147000 iteration, current mean episode reward is 0.536.\n",
      "0.021130296507418725\n",
      "[INFO]\tIn 148000 iteration, current mean episode reward is 0.197.\n",
      "[INFO]\tIn 149000 iteration, current mean episode reward is 0.146.\n",
      "0.02007378168204779\n",
      "[INFO]\tIn 150000 iteration, current mean episode reward is 0.639.\n",
      "[INFO]\tIn 151000 iteration, current mean episode reward is 0.661.\n",
      "0.0190700925979454\n",
      "[INFO]\tIn 152000 iteration, current mean episode reward is 0.534.\n",
      "[INFO]\tIn 153000 iteration, current mean episode reward is 0.656.\n",
      "0.018116587968048128\n",
      "[INFO]\tIn 154000 iteration, current mean episode reward is 0.476.\n",
      "[INFO]\tIn 155000 iteration, current mean episode reward is 0.349.\n",
      "0.01721075856964572\n",
      "[INFO]\tIn 156000 iteration, current mean episode reward is 0.496.\n",
      "[INFO]\tIn 157000 iteration, current mean episode reward is 0.522.\n",
      "0.016350220641163433\n",
      "[INFO]\tIn 158000 iteration, current mean episode reward is 0.561.\n",
      "[INFO]\tIn 159000 iteration, current mean episode reward is 0.394.\n",
      "0.015532709609105261\n",
      "[INFO]\tIn 160000 iteration, current mean episode reward is 0.63.\n",
      "[INFO]\tIn 161000 iteration, current mean episode reward is 0.636.\n",
      "0.014756074128649998\n",
      "[INFO]\tIn 162000 iteration, current mean episode reward is 0.217.\n",
      "[INFO]\tIn 163000 iteration, current mean episode reward is 0.604.\n",
      "0.014018270422217498\n",
      "[INFO]\tIn 164000 iteration, current mean episode reward is 0.644.\n",
      "[INFO]\tIn 165000 iteration, current mean episode reward is 0.488.\n",
      "0.013317356901106622\n",
      "[INFO]\tIn 166000 iteration, current mean episode reward is 0.74.\n",
      "[INFO]\tIn 167000 iteration, current mean episode reward is 0.658.\n",
      "0.01265148905605129\n",
      "[INFO]\tIn 168000 iteration, current mean episode reward is 0.625.\n",
      "[INFO]\tIn 169000 iteration, current mean episode reward is 0.647.\n",
      "0.012018914603248726\n",
      "[INFO]\tIn 170000 iteration, current mean episode reward is 0.698.\n",
      "[INFO]\tIn 171000 iteration, current mean episode reward is 0.642.\n",
      "0.01141796887308629\n",
      "[INFO]\tIn 172000 iteration, current mean episode reward is 0.595.\n",
      "[INFO]\tIn 173000 iteration, current mean episode reward is 0.48.\n",
      "0.010847070429431975\n",
      "[INFO]\tIn 174000 iteration, current mean episode reward is 0.582.\n",
      "[INFO]\tIn 175000 iteration, current mean episode reward is 0.473.\n",
      "0.010304716907960376\n",
      "[INFO]\tIn 176000 iteration, current mean episode reward is 0.24.\n",
      "[INFO]\tIn 177000 iteration, current mean episode reward is 0.497.\n",
      "0.009789481062562356\n",
      "[INFO]\tIn 178000 iteration, current mean episode reward is 0.534.\n",
      "[INFO]\tIn 179000 iteration, current mean episode reward is 0.669.\n",
      "0.009300007009434237\n",
      "[INFO]\tIn 180000 iteration, current mean episode reward is 0.396.\n",
      "[INFO]\tIn 181000 iteration, current mean episode reward is 0.567.\n",
      "0.008835006658962525\n",
      "[INFO]\tIn 182000 iteration, current mean episode reward is 0.176.\n",
      "[INFO]\tIn 183000 iteration, current mean episode reward is 0.502.\n",
      "0.008393256326014398\n",
      "[INFO]\tIn 184000 iteration, current mean episode reward is 0.606.\n",
      "[INFO]\tIn 185000 iteration, current mean episode reward is 0.612.\n",
      "0.007973593509713677\n",
      "[INFO]\tIn 186000 iteration, current mean episode reward is 0.673.\n",
      "[INFO]\tIn 187000 iteration, current mean episode reward is 0.468.\n",
      "0.007574913834227992\n",
      "[INFO]\tIn 188000 iteration, current mean episode reward is 0.283.\n",
      "[INFO]\tIn 189000 iteration, current mean episode reward is 0.695.\n",
      "0.0071961681425165925\n",
      "[INFO]\tIn 190000 iteration, current mean episode reward is 0.678.\n",
      "[INFO]\tIn 191000 iteration, current mean episode reward is 0.43.\n",
      "0.006836359735390762\n",
      "[INFO]\tIn 192000 iteration, current mean episode reward is 0.07.\n",
      "[INFO]\tIn 193000 iteration, current mean episode reward is 0.667.\n",
      "0.0064945417486212235\n",
      "[INFO]\tIn 194000 iteration, current mean episode reward is 0.76.\n",
      "[INFO]\tIn 195000 iteration, current mean episode reward is 0.654.\n",
      "0.006169814661190162\n",
      "[INFO]\tIn 196000 iteration, current mean episode reward is 0.732.\n",
      "[INFO]\tIn 197000 iteration, current mean episode reward is 0.702.\n",
      "0.005861323928130653\n",
      "[INFO]\tIn 198000 iteration, current mean episode reward is 0.693.\n",
      "[INFO]\tIn 199000 iteration, current mean episode reward is 0.712.\n",
      "0.00556825773172412\n",
      "[INFO]\tIn 200000 iteration, current mean episode reward is 0.553.\n",
      "[INFO]\tIn 201000 iteration, current mean episode reward is 0.666.\n",
      "0.005289844845137914\n",
      "[INFO]\tIn 202000 iteration, current mean episode reward is 0.639.\n",
      "[INFO]\tIn 203000 iteration, current mean episode reward is 0.674.\n",
      "0.005025352602881018\n",
      "[INFO]\tIn 204000 iteration, current mean episode reward is 0.525.\n",
      "[INFO]\tIn 205000 iteration, current mean episode reward is 0.778.\n",
      "0.004774084972736967\n",
      "[INFO]\tIn 206000 iteration, current mean episode reward is 0.72.\n",
      "[INFO]\tIn 207000 iteration, current mean episode reward is 0.7.\n",
      "0.004535380724100119\n",
      "[INFO]\tIn 208000 iteration, current mean episode reward is 0.569.\n",
      "[INFO]\tIn 209000 iteration, current mean episode reward is 0.657.\n",
      "0.004308611687895113\n",
      "[INFO]\tIn 210000 iteration, current mean episode reward is 0.513.\n",
      "[INFO]\tIn 211000 iteration, current mean episode reward is 0.702.\n",
      "0.004093181103500357\n",
      "[INFO]\tIn 212000 iteration, current mean episode reward is 0.416.\n",
      "[INFO]\tIn 213000 iteration, current mean episode reward is 0.565.\n",
      "0.003888522048325339\n",
      "[INFO]\tIn 214000 iteration, current mean episode reward is 0.466.\n",
      "[INFO]\tIn 215000 iteration, current mean episode reward is 0.68.\n",
      "0.003694095945909072\n",
      "[INFO]\tIn 216000 iteration, current mean episode reward is 0.576.\n",
      "[INFO]\tIn 217000 iteration, current mean episode reward is 0.56.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0035093911486136185\n",
      "[INFO]\tIn 218000 iteration, current mean episode reward is 0.537.\n",
      "[INFO]\tIn 219000 iteration, current mean episode reward is 0.515.\n",
      "0.0033339215911829373\n",
      "[INFO]\tIn 220000 iteration, current mean episode reward is 0.513.\n",
      "[INFO]\tIn 221000 iteration, current mean episode reward is 0.763.\n",
      "0.0031672255116237903\n",
      "[INFO]\tIn 222000 iteration, current mean episode reward is 0.367.\n",
      "[INFO]\tIn 223000 iteration, current mean episode reward is 0.56.\n",
      "0.003008864236042601\n",
      "[INFO]\tIn 224000 iteration, current mean episode reward is 0.655.\n",
      "[INFO]\tIn 225000 iteration, current mean episode reward is 0.235.\n",
      "0.0028584210242404706\n",
      "[INFO]\tIn 226000 iteration, current mean episode reward is 0.507.\n",
      "[INFO]\tIn 227000 iteration, current mean episode reward is 0.485.\n",
      "0.002715499973028447\n",
      "[INFO]\tIn 228000 iteration, current mean episode reward is 0.524.\n",
      "[INFO]\tIn 229000 iteration, current mean episode reward is 0.698.\n",
      "0.0025797249743770246\n",
      "[INFO]\tIn 230000 iteration, current mean episode reward is 0.779.\n",
      "[INFO]\tIn 231000 iteration, current mean episode reward is 0.679.\n",
      "0.0024507387256581734\n",
      "[INFO]\tIn 232000 iteration, current mean episode reward is 0.677.\n",
      "[INFO]\tIn 233000 iteration, current mean episode reward is 0.527.\n",
      "0.0023282017893752646\n",
      "[INFO]\tIn 234000 iteration, current mean episode reward is 0.268.\n",
      "[INFO]\tIn 235000 iteration, current mean episode reward is 0.529.\n",
      "0.002211791699906501\n",
      "[INFO]\tIn 236000 iteration, current mean episode reward is 0.483.\n",
      "[INFO]\tIn 237000 iteration, current mean episode reward is 0.5.\n",
      "0.002101202114911176\n",
      "[INFO]\tIn 238000 iteration, current mean episode reward is 0.724.\n",
      "[INFO]\tIn 239000 iteration, current mean episode reward is 0.591.\n",
      "0.0019961420091656173\n",
      "[INFO]\tIn 240000 iteration, current mean episode reward is 0.635.\n",
      "[INFO]\tIn 241000 iteration, current mean episode reward is 0.44.\n",
      "0.0018963349087073363\n",
      "[INFO]\tIn 242000 iteration, current mean episode reward is 0.623.\n",
      "[INFO]\tIn 243000 iteration, current mean episode reward is 0.656.\n",
      "0.0018015181632719695\n",
      "[INFO]\tIn 244000 iteration, current mean episode reward is 0.269.\n",
      "[INFO]\tIn 245000 iteration, current mean episode reward is 0.704.\n",
      "0.001711442255108371\n",
      "[INFO]\tIn 246000 iteration, current mean episode reward is 0.566.\n",
      "[INFO]\tIn 247000 iteration, current mean episode reward is 0.662.\n",
      "0.0016258701423529523\n",
      "[INFO]\tIn 248000 iteration, current mean episode reward is 0.674.\n",
      "[INFO]\tIn 249000 iteration, current mean episode reward is 0.675.\n"
     ]
    }
   ],
   "source": [
    "# It's ok to leave this cell commented.\n",
    "\n",
    "new_config = dict(\n",
    "    env_name=\"FrozenLake8x8-v0\",\n",
    "  \n",
    "    max_iteration=250000,\n",
    "    max_episode_length=10000,\n",
    "    evaluate_interval=1000,\n",
    "    gamma=0.9,\n",
    "    eps=0.99,\n",
    "    learning_rate=0.5\n",
    "    \n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# new_mc_control_trainer = mc_control(new_config)\n",
    "# new_q_learning_trainer = q_learning(new_config)\n",
    "new_sarsa_trainer = sarsa(new_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have implement the MC Control algorithm. You have finished this section. If you want to do more investigation like comparing the policy provided by SARSA, Q-Learning and MC Control, then you can do it in the next cells. It's OK to leave it blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can do more investigation here if you wish. Leave it blank if you don't.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Conclusion and Discussion\n",
    "\n",
    "It's OK to leave the following cells empty. In the next markdown cell, you can write whatever you like. Like the suggestion on the course, the confusing problems in the assignments, and so on.\n",
    "\n",
    "If you want to do more investigation, feel free to open new cells via `Esc + B` after the next cells and write codes in it, so that you can reuse some result in this notebook. Remember to write sufficient comments and documents to let others know what you are doing.\n",
    "\n",
    "Following the submission instruction in the assignment to submit your assignment to our staff. Thank you!\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
